{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Predictions for car battery life"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Problem overview:"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This scenario focuses on predictions for car batteries reaching their rated number of cycles. The goal here is to avoid a sudden failure of the car battery and schedule it to be replaced before it reaches its rated number of cycles. The reduces significantly the risk of collateral failures occuring as well. From a business point of view, the target is optimize car battery replacement schedules to minimize overall maintenance costs. \n\nThe connected cars report on a daily basis information about the number of trips, duration of those trips and number of battery cycles used. Since reporting is performed on a daily basis, the battery age measured in days is also available as an input. The accumulated lifetime cycles used measure is also part of the dataset. As you can easily guess, the input data comes in the form of timeseries with the **Lifetime_Cycles_Used** field being the subject of prediction. Given the current timeseries for each battery, we are interested in forecasting the value of this field for the next 30 days to see whether the rated number of cycles will be exceeeded or not. \n\nThis scenario details the development of a machine learning time series forecasting model. The model is trained on a dataset containing battery telemetry information for 1539 days (between 1/1/2013 and 3/19/2017) for one car battery."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Solution overview:"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We will use the [Automated Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-auto-train) (autoML) capabilities of the [Azure Machine Learning service](https://docs.microsoft.com/en-us/azure/machine-learning/service/overview-what-is-azure-ml) to quickly train a model that can forecast the evolution of the **Lifetime_Cycles_Used** metric. We will model our problem as a **Forecasting** problem where the goal of the trained model is to forecast the future evolution of a numerical indicator over a number of cycles (30 days in our case). The automML capabilities enable us to evaluate different algorithms and hyperparameters to get the best trained model for the problem with minimum effort. The approach used in this example cand be extended to various use cases that revolve around the need to predict the time series-based evolution of a numerical value.\n\n\nThis notebook is organized into the following sections:\n\n1. Basic setup\n\n2. Data prep\n\n3. Model training\n\n4. Explore the results and evaluate the best model\n\n5. Model Explainability: which features matter for the predictions?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Section 1. Basic setup\n\nBefore starting this step, you need to create an Azure Machine Learning service workspace ([instructions](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-workspace)).\n\nLet's get started by creating an experiment in your Azure Machine Learning workspace. An experiment is a named object in a workspace, which is used to do model training."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import azureml.core\nimport pandas as pd\nimport numpy as np\nimport logging\nimport warnings\n# Squash warning messages for cleaner output in the notebook\nwarnings.showwarning = lambda *args, **kwargs: None\n\n\nfrom azureml.core.workspace import Workspace\nfrom azureml.core.experiment import Experiment\nfrom azureml.train.automl import AutoMLConfig\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "subscription_id = \"<subscription id goes here>\"\nresource_group = \"<resource group goes here>\"\nworkspace_region = \"<workspace region goes here>\"\nworkspace_name = \"<workspace name goes here>\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ws = Workspace(workspace_name = workspace_name,\n               subscription_id = subscription_id,\n               resource_group = resource_group)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# choose a name for the run history container in the workspace\nexperiment_name = 'CarBatteryLife'\n\n# project folder\nproject_folder = './sample_projects/automl-carbatterylifeforecast'\n\nexperiment=Experiment(ws, experiment_name)\n\noutput = {}\noutput['SDK version'] = azureml.core.VERSION\noutput['Subscription ID'] = ws.subscription_id\noutput['Workspace Name'] = ws.name\noutput['Resource Group'] = ws.resource_group\noutput['Location'] = ws.location\noutput['Project Directory'] = project_folder\noutput['Experiment Name'] = experiment.name\npd.set_option('display.max_colwidth', -1)\npd.DataFrame(data = output, index = ['']).T",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Section 2. Data prep\n\nOnlineRetail.csv contains the customer purchase history between December 1st, 2010 and December 9th, 2011."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "data = pd.read_csv(\"daily-battery-time-series.csv\", parse_dates=['Date'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 2.1 Inspect data\n\nDisplay the first few rows of the data and view some plots to help you understand the dynamics within the dataset."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data.head(20)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Plot to be added here",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Plot to be added here",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 2.2 Split the data into train and test sets\n\nWhen working with timeseries, the approach used for split is to use the first X% occurences from the time series as training data and the remaining (100-X)% occurences as test data. Once the forecasting model is trained, it is used to make predictions for the occurrences from the test data. The differences between the predicted values and the actual values from the test data are used to measure the performance of the model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# let's take note of what columns means what in the data\ntime_column_name = 'Date'\ntarget_column_name = 'Lifetime_Cycles_Used'\n\nX_train = data[data[time_column_name] < '2016-04-15']\nX_test = data[data[time_column_name] >= '2016-04-15']\ny_train = X_train.pop(target_column_name).values\ny_test = X_test.pop(target_column_name).values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Section 3. Model training\n\nIn this section you will configure the [automated ML](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-automated-ml) feature of the Azure Machine Learning service. Using the configuration you will then run an automated ML experiment which explores various algorithms and hyperparameter values to generate machine learning models. Finally, the best model will be selected. The training jobs run on local compute resources (provided and managed by Azure Notebooks).\n\nThe significant advantage of automated ML is the acceleration of a data scientist's work (as it does a significant portion of the exploration work). Besides that, automated ML exposes rich data resulting from experiment runs which enables control, transparency, and, most importantly, visibility on what is happening behind the scenes.\n\nThe configuration data required by automated ML contains information about the experiment itself as well as the training data used to train the models. Below is an example of the most important components of the configuration data:\n\nProperty | Description\n--- | ---\ntask | regression\nprimary_metric | Metric that you want to optimize.<br>Forecasting supports the following primary metrics:<br>spearman_correlation<br>normalized_root_mean_squared_error<br>r2_score<br>normalized_mean_absolute_error\niterations | Number of iterations. In each individual iteration, automated ML trains one pipeline (algorithm and hyperparameters) on the given data.\niteration_timeout_minutes | The maximum number of minutes for each individual iteration\nX | Training data in the form [n_samples, n_features]\ny | Target values in the form [n_samples]\nn_cross_validations | Number of [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) splits.\npath | Relative path to the project folder. Automated ML stores configuration files for the experiment under this folder. You can specify a new empty folder.\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.1 Automated ML configuration\n\nAutomated ML provides several options to configure the experiment runs, giving you flexibility and control. For example, the primary_metric setting specifies which metrich should automated ML use to optimize the machine learning model being built. There are multiple primary metrics available, and in our problem we will use NRMSE (Normalized Root Mean Squared Error).\n\nNotice that we've set the task to forecasting and we are also specifying the training data set (X_train and y_train). We need to do this because training is performed locally. When training in performed remotely (e.g. on AML compute resources) you will need to provide a script that contains code to get the data instead of the data itself.\n\nAlso, we are indicating the time column name and the maximum time horizon for the forecast (30 cycles which amount to 30 days in our case)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "automl_settings = {\n    \"time_column_name\": time_column_name, \n    \"max_horizon\": 30\n}\n\nautoml_config = AutoMLConfig(task = 'forecasting',\n                             debug_log = 'automl_carbatterylife_errors.log',\n                             primary_metric= 'normalized_root_mean_squared_error',\n                             iterations = 5,\n                             iteration_timeout_minutes = 5,\n                             X = X_train,\n                             y = y_train,\n                             n_cross_validations = 3,\n                             path=project_folder,\n                             verbosity = logging.INFO,\n                            **automl_settings)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.2 Train your models on local compute\n\nWhen you call the submit method on the experiment object and pass the AutoMLConfig object, automated ML will general a number of machine learning models equal to **iterations** (5 in our case). Depending on the input data and the number of iterations the time required to complete the traning can range from a few minutes to hours (or even mode). Once execution starts, you will see status messages being print out to the console."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "local_run = experiment.submit(automl_config, show_output=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.3 Monitor training"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "When you want to start the run and continue to execute your code you need to specify ```show_output=False``` when calling experiment.submit. This will also enable you to use a widget to view the status of all iterations (see the cell below)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\nRunDetails(local_run).show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Section 4. Explore the results and test the best model"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 4.1 Retrieve all child runs\n\nEach individual model is trained in the context of a child run having **local_run** as its parent. You can get the list of all child runs and their logged metrics."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "children = list(local_run.get_children())\nmetricslist = {}\nfor run in children:\n    properties = run.get_properties()\n    metrics = {k: v for k, v in run.get_metrics().items() if isinstance(v, float)}    \n    metricslist[int(properties['iteration'])] = metrics\n\nrundata = pd.DataFrame(metricslist).sort_index(1)\nrundata",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 4.2 Retrieve the best fitted model\n\nThe **get_output** method enables you to retrieve the best child run and the associated fitted model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_run, fitted_model = local_run.get_output()\nfitted_model.steps",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 4.3 Test the best fitted model\n\nDescription of the process to be added."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "y_pred_test = fitted_model.predict(X_test)\ny_residual_test = y_test - y_pred_test\n\nplt.plot(X_test['Date'], y_test, label='Actual')\nplt.plot(X_test['Date'], y_pred_test, label=\"Predicted\")\nplt.xticks(rotation=90)\nplt.title('Actual battery life vs predicted for test data ')\nplt.legend()\nplt.show()",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Section 5. Model Explainability: Which features matter for the predictions?\n\nDescription of the process to be added"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Model explainability code to be added",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}