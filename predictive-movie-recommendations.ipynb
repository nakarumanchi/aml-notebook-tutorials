{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Predictive Movie Recommendations"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Problem Overview"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This scenario focuses on predictions related to movie recommendations where the goal is to predict the rating a user would give to a movie the user never rated before. Using this value, you can identify the list of most interesting movies for a particular user and recommend those movies. The dataset we are working with in this scenario consists of user actions related to movies:\n\n- ```details```: the user viewed the details of the movie\n- ```addToCart```: the user added the movie to the shopping cart\n- ```buy```: the user bought the movie \n\nOne of the most widely used appproaches to solve this kind of problem is Collaborative Filtering. Collaborative Filtering is a technique based on the assumption that if two users have the same taste or opinion on a given issue, then itâ€™s more likely that those two users will share the same taste or opinion on a different/new issue. There are multiple types of collaborative filtering: memory-based, model-based, hybrid, and deep learning-based. In this notebook we will showcase one particular model-based collaborative filtering algorithm - SVD (Singular Value Decomposition). The ```scikit-surprise``` library provides a wide range of collaborative filtering algorithms but you can use any other library instead of it.\n\nOne important aspect to notice is the input dataset does not contain explicit ratings (which are required by the SVD algorithm). Still, the data contains information that can be translated into ratings using a fairly simple approach (in this case we call the ratings implicit ratings). \n\nThe scenario details the development of a machine learning movie rating prediction model. The model is trained on a dataset containing movie-related user actions. As part of the development process, the calculation of implicit ratings is also showcased."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Solution Overview"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We will use the remote model training capabilities of the [Azure Machine Learning service](https://docs.microsoft.com/en-us/azure/machine-learning/service/overview-what-is-azure-ml). The remote compute resources used will be provided by [Azure Machine Learning Compute](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute). We will model our problem as a model-based collaborative filtering problem where the goal of the trained model is to predict the rating a user would give to a movie. These predictions can then be used to rank movies for that user and recommend the most interesting ones.\n\nThis notebook is organized into the following sections:\n\n1. Basic setup\n\n2. Data prep\n\n3. Model training\n\n4. Load the trained model and make movie recommendations"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Section 1. Basic setup"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Make sure the ```scikit-surprise``` (Simple Python RecommendatIon System Engine) library is installed. This will be provide the recommender algorithm used in this notebook."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install --upgrade scikit-surprise",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Make the necessary namespace and class imports."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport logging\nimport warnings\nimport os\n# Squash warning messages for cleaner output in the notebook\nwarnings.showwarning = lambda *args, **kwargs: None\n\nfrom matplotlib import pyplot as plt\n\nfrom surprise import dump\n\nimport azureml.core\nfrom azureml.core.workspace import Workspace\nfrom azureml.core.experiment import Experiment\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\nfrom azureml.core.runconfig import RunConfiguration\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.runconfig import DEFAULT_CPU_IMAGE\nfrom azureml.core import ScriptRunConfig",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Before starting this step, you need to create an Azure Machine Learning service workspace ([instructions](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-workspace)).\n\nLet's get started by creating an experiment in your Azure Machine Learning workspace. An experiment is a named object in a workspace, which is used to do model training."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "subscription_id = \"<subscription id goes here>\"\nresource_group = \"<resource group goes here>\"\nworkspace_name = \"<workspace name goes here>\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ws = Workspace(workspace_name = workspace_name,\n               subscription_id = subscription_id,\n               resource_group = resource_group)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# choose a name for the run history container in the workspace\nexperiment_name = 'PredictiveMovieRecommendations'\n\n# project folder\nproject_folder = './sample_projects/predictivemovierecommendations'\nos.makedirs(project_folder, exist_ok=True)\n\nexperiment=Experiment(ws, experiment_name)\n\noutput = {}\noutput['SDK version'] = azureml.core.VERSION\noutput['Subscription ID'] = ws.subscription_id\noutput['Workspace Name'] = ws.name\noutput['Resource Group'] = ws.resource_group\noutput['Location'] = ws.location\noutput['Project Directory'] = project_folder\noutput['Experiment Name'] = experiment.name\npd.set_option('display.max_colwidth', -1)\npd.DataFrame(data = output, index = ['']).T",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Section 2. Data prep\n\nMovie_Customer_Actions.csv contains 4000 movie-related customer actions (details, addToCart, and buy) performed during various sessions."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data = pd.read_csv(\"https://quickstartsws9073123377.blob.core.windows.net/amlnotebooktutorials/movie-customer-actions/movie_customer_actions.csv\")\ndata.head(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 2.1 Calculate implicit ratings\n\nThe dataset records movie-related customer actions which do not contain explicit ratings for the movies. The first step we need to perform is to determine these ratings using a simple approach: for each combination of MovieId and UserId we count the occurences of each action (details, addToCart, buy), combine them using a predefined set of weights, and then normalize the results to get a 0 to 10 scale.\n\nFirst, let's count the actions:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "counts = data.groupby(['MovieId', 'UserId', 'Action'])['Action'].count()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "agg_data = dict()\n\nfor i in counts.index:\n    \n    if i[0] not in agg_data.keys():\n        \n        movie_agg_data = dict()\n        user_agg_data = {\n            'buy': 0,\n            'addToCart': 0,\n            'details': 0\n        }\n        movie_agg_data[i[1]] = user_agg_data\n        agg_data[i[0]] = movie_agg_data\n        \n    else:\n        \n        movie_agg_data = agg_data[i[0]]\n        if i[1] not in movie_agg_data.keys():\n            user_agg_data = {\n            'buy': 0,\n            'addToCart': 0,\n            'details': 0\n        }\n        movie_agg_data[i[1]] = user_agg_data\n        \n    agg_data[i[0]][i[1]][i[2]] += counts[i]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Using the predefined weights, calculate the raw rating for each MovieId, UserId pair."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "weights = {\n    'buy': 100,\n    'addToCart': 50,\n    'details': 15\n}\n\nraw_ratings = []\nmax_rating = 0\n\nfor movie_key in agg_data.keys():\n    for user_key in agg_data[movie_key].keys():\n        raw_rating = 0\n        for weights_key in weights.keys():\n            raw_rating += agg_data[movie_key][user_key][weights_key] * weights[weights_key]\n        if raw_rating > max_rating:\n            max_rating = raw_rating\n        raw_ratings.append([movie_key, user_key, raw_rating])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finally, normalize the ratings to get a scale from 0 to 10."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ratings = pd.DataFrame(raw_ratings, columns=['MovieId', 'UserId', 'RawRating'])\nratings['Rating'] = float(10) * ratings['RawRating'] / max_rating\nratings.head(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Section 3. Model training"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "First, make sure the necessary compute resources are available. If you want to reuse an existing Azure Machine Learning compute cluster, change the value of the ```cpu_cluster_name``` variable below."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Choose a name for your CPU cluster\ncpu_cluster_name = \"aml-compute-01\"\n\n# Verify that cluster does not exist already\ntry:\n    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n    print('Found existing cluster, use it.')\nexcept ComputeTargetException:\n    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n                                                           max_nodes=1)\n    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n\ncpu_cluster.wait_for_completion(show_output=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Create a run configuration for the compute target. Notice the dependency that is specified: ```scikit-surprise```."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create a new runconfig object\nrun_amlcompute = RunConfiguration()\n\n# Use the cpu_cluster you created above. \nrun_amlcompute.target = cpu_cluster\n\n# Enable Docker\nrun_amlcompute.environment.docker.enabled = True\n\n# Set Docker base image to the default CPU-based image\nrun_amlcompute.environment.docker.base_image = DEFAULT_CPU_IMAGE\n\n# Use conda_dependencies.yml to create a conda environment in the Docker image for execution\nrun_amlcompute.environment.python.user_managed_dependencies = False\n\n# Specify CondaDependencies obj, add necessary packages\nrun_amlcompute.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['scikit-surprise'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Create the training script that will be submitted to the remote AML compute cluster. The content of the training script is based on the logic we have shown so far (load data -> calculate implicit ratings -> normalize ratings) completed with the use of the SVD (Singular Value Decomposition) algorithm from ```scikit-surprise```.\n\nThe ```scikit-surprise``` library supports many more recommendation algorithms:\n- Basic: NormalPredictor, BaselineOnly\n- k-NN: KNNBasic, KNNWithMeans, KNNWithZScore, KNNBaseline\n- Matrix factorization: SVD, SVDpp, NMF, SlopeOne, CoClustering\n\nMore details about these are available in the [scikit-suprise documentation](https://surprise.readthedocs.io/en/stable/).\n\nThe last step performed by the training script is to write the trained model in the ```outputs``` folder. With AML, anything that is written to that folder during the training process becomes available as part of the experiment run record. This allows you do download the trained model at any later point in time and use it to make predictions."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile $project_folder/train.py\n\nimport pandas as pd\nfrom surprise import SVD\nfrom surprise import Dataset\nfrom surprise import Reader\nfrom surprise import accuracy\nfrom surprise import dump\nfrom surprise.model_selection import train_test_split\n\nfrom azureml.core import Run\n\ndata = pd.read_csv(\"https://quickstartsws9073123377.blob.core.windows.net/amlnotebooktutorials/movie-customer-actions/movie_customer_actions.csv\")\ncounts = data.groupby(['MovieId', 'UserId', 'Action'])['Action'].count()\n\nagg_data = dict()\n\nfor i in counts.index:\n    \n    if i[0] not in agg_data.keys():\n        \n        movie_agg_data = dict()\n        user_agg_data = {\n            'buy': 0,\n            'addToCart': 0,\n            'details': 0\n        }\n        movie_agg_data[i[1]] = user_agg_data\n        agg_data[i[0]] = movie_agg_data\n        \n    else:\n        \n        movie_agg_data = agg_data[i[0]]\n        if i[1] not in movie_agg_data.keys():\n            user_agg_data = {\n            'buy': 0,\n            'addToCart': 0,\n            'details': 0\n        }\n        movie_agg_data[i[1]] = user_agg_data\n        \n    agg_data[i[0]][i[1]][i[2]] += counts[i]\n    \nweights = {\n    'buy': 100,\n    'addToCart': 50,\n    'details': 15\n}\n\nraw_ratings = []\nmax_rating = 0\n\nfor movie_key in agg_data.keys():\n    for user_key in agg_data[movie_key].keys():\n        raw_rating = 0\n        for weights_key in weights.keys():\n            raw_rating += agg_data[movie_key][user_key][weights_key] * weights[weights_key]\n        if raw_rating > max_rating:\n            max_rating = raw_rating\n        raw_ratings.append([movie_key, user_key, raw_rating])\n        \nratings = pd.DataFrame(raw_ratings, columns=['MovieId', 'UserId', 'RawRating'])\nratings['Rating'] = float(10) * ratings['RawRating'] / max_rating\n\nreader = Reader(rating_scale=(0, 10))\ndata = Dataset.load_from_df(ratings[['UserId', 'MovieId', 'Rating']], reader)\n\n# sample random trainset and testset\n# test set is made of 25% of the ratings.\ntrainset, testset = train_test_split(data, test_size=.25)\n\n# We'll use the famous SVD algorithm.\nalgo = SVD()\n\n# Train the algorithm on the trainset, and predict ratings for the testset\nalgo.fit(trainset)\npredictions = algo.test(testset)\n\n# Then compute RMSE\nrmse = accuracy.rmse(predictions)\n\n# Get the containing AML run and log the metric\nrun = Run.get_context()\nrun.log('rmse', rmse)\n\n# Save the trained model into the outputs to make sure is automatically uploaded into experiment record\ndump.dump('outputs/movie_recommender.pkl', algo=algo)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finally, submit your experiment run using the ```train.py``` file you've just created. \n\nNotice the url displayed right after execution starts - this allows you to view the progress of the run directly in the AML portal."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "src = ScriptRunConfig(source_directory = project_folder, script = 'train.py', run_config = run_amlcompute)\nrun = experiment.submit(src)\nrun.wait_for_completion(show_output = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can also check the details of the experiment run that has just finished. \n\nNotice the urls on the right side - the link to the AML portal and the link to the AML documentation."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Section 4. Load the trained model and make movie recommendations"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The trained recommender model is now available in the experiment run record and can be downloaded."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run.download_file('outputs/movie_recommender.pkl')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can now load the trained model which will be available in the ```algo``` variable."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pred, algo = dump.load('movie_recommender.pkl')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, we will attempt to find a MovieId, UserId pair that does not exists in the ```ratings``` data frame. To do that, we will choose a UserId (400001) and find all the MovieId values that have been rated by other users and have not been rated by this user."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "set(ratings.MovieId.unique()).difference(ratings[ratings.UserId == 400001].MovieId.unique())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Verify the fact that there is not rating for MovieId 2404435 by UserId 400001."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ratings[(ratings.UserId == 400001) & (ratings.MovieId == 2404435)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We've reached now our final step - predicting the rating for a movie that hasn't been rated previously by a given user.\n\nNotice the ```est``` value which is the rating we are looking for."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "algo.predict(400001, 2404435)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}