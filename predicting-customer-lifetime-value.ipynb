{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Predictions for Customer Lifetime Value "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Problem overview:"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This scenario focuses on predictions related to CLV (Customer Lifetime Value) where the goal is to predict the future value a customer brings to the business. For any marketing department, the CLV is a key metric that helps understand the revenue attached to the future relationship with the customer. In general, the CLV is used to optimize marketing spends while maximizing returns. While there are many ways to model the CLV, in this scenario we will use a metric related to future customer spend in a given time range (e.g. a quarter). Predicting the future spend relies on the known transaction history of customers. One of the most widely used models to capture customer behavior is RFM (Recency, Frequency, Monetary). RFM features are derived from the transactional history of the customers and then used as inputs for the machine learning models that predict future spend.\n\nThis scenario details the development of a machine learning future customer spend prediction model. The model is trained on a public dataset containing transactions from an online retailer. The dataset contains the history of purchased items over 12 calendar months."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Solution overview:"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We will use the [Automated Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-auto-train) (autoML) capabilities of the [Azure Machine Learning service](https://docs.microsoft.com/en-us/azure/machine-learning/service/overview-what-is-azure-ml) to quickly train a model that can predict future customer spend. We will model our problem as a **Regression** problem where the goal of the trained model is to predict a numerical value (in our case, the future customer spend). The automML capabilities enable us to evaluate different algorithms and hyperparameters to get the best trained model for the problem with minimum effort. The approach used in this example cand be extended to various use cases that revolve around the need to predict numerical values related to customer behavior (and not only).\n\n\nThis notebook is organized into the following sections:\n\n1. Basic setup\n\n2. Data prep\n\n3. Model training\n\n4. Explore the results and evaluate the best model\n\n5. Model Explainability: which features matter for the predictions?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Section 1. Basic setup\n\nBefore starting this step, you need to create an Azure Machine Learning service workspace ([instructions](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-workspace)).\n\nLet's get started by creating an experiment in your Azure Machine Learning workspace. An experiment is a named object in a workspace, which is used to do model training."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import azureml.core\nimport pandas as pd\nimport numpy as np\nimport logging\nimport warnings\n# Squash warning messages for cleaner output in the notebook\nwarnings.showwarning = lambda *args, **kwargs: None\n\n\nfrom azureml.core.workspace import Workspace\nfrom azureml.core.experiment import Experiment\nfrom azureml.train.automl import AutoMLConfig\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "subscription_id = \"<subscription id goes here>\"\nresource_group = \"<resource group goes here>\"\nworkspace_region = \"<workspace region goes here>\"\nworkspace_name = \"<workspace name goes here>\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ws = Workspace(workspace_name = workspace_name,\n               subscription_id = subscription_id,\n               resource_group = resource_group)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# choose a name for the run history container in the workspace\nexperiment_name = 'CLVFutureSpend'\n\n# project folder\nproject_folder = './sample_projects/automl-clvfuturespendprediction'\n\nexperiment=Experiment(ws, experiment_name)\n\noutput = {}\noutput['SDK version'] = azureml.core.VERSION\noutput['Subscription ID'] = ws.subscription_id\noutput['Workspace Name'] = ws.name\noutput['Resource Group'] = ws.resource_group\noutput['Location'] = ws.location\noutput['Project Directory'] = project_folder\noutput['Experiment Name'] = experiment.name\npd.set_option('display.max_colwidth', -1)\npd.DataFrame(data = output, index = ['']).T",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Section 2. Data prep\n\nOnlineRetail.csv contains the customer purchase history between December 1st, 2010 and December 9th, 2011."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "data = pd.read_csv(\"OnlineRetail.csv\", parse_dates=['timeStamp'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 2.1 Inspect data\n\nDisplay the first few rows of the data and view some plots to help you understand the dynamics within the dataset."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data.head(20)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Plot to be added here",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Plot to be added here",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 2.2 Engineer new features - Recency, Frequency, and Monetary"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# RFM calculation to be added here",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 2.3 Split the data into train and test sets"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Train/Test split to be added here",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Section 3. Model training\n\nIn this section you will configure the [automated ML](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-automated-ml) feature of the Azure Machine Learning service. Using the configuration you will then run an automated ML experiment which explores various algorithms and hyperparameter values to generate machine learning models. Finally, the best model will be selected. The training jobs run on local compute resources (provided and managed by Azure Notebooks).\n\nThe significant advantage of automated ML is the acceleration of a data scientist's work (as it does a significant portion of the exploration work). Besides that, automated ML exposes rich data resulting from experiment runs which enables control, transparency, and, most importantly, visibility on what is happening behind the scenes.\n\nThe configuration data required by automated ML contains information about the experiment itself as well as the training data used to train the models. Below is an example of the most important components of the configuration data:\n\nProperty | Description\n--- | ---\ntask | regression\nprimary_metric | Metric that you want to optimize.<br>Forecasting supports the following primary metrics:<br>spearman_correlation<br>normalized_root_mean_squared_error<br>r2_score<br>normalized_mean_absolute_error\niterations | Number of iterations. In each individual iteration, automated ML trains one pipeline (algorithm and hyperparameters) on the given data.\niteration_timeout_minutes | The maximum number of minutes for each individual iteration\nX | Training data in the form [n_samples, n_features]\ny | Target values in the form [n_samples]\nn_cross_validations | Number of [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) splits.\npath | Relative path to the project folder. Automated ML stores configuration files for the experiment under this folder. You can specify a new empty folder.\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.1 Automated ML configuration\n\nAutomated ML provides several options to configure the experiment runs, giving you flexibility and control. For example, the primary_metric setting specifies which metrich should automated ML use to optimize the machine learning model being built. There are multiple primary metrics available, and in our problem we will use NRMSE (Normalized Root Mean Squared Error).\n\nNotice that we've set the task to regression and we are also specifying the training data set (X_train and y_train). We need to do this because training is performed locally. When training in performed remotely (e.g. on AML compute resources) you will need to provide a script that contains code to get the data instead of the data itself."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "automl_config = AutoMLConfig(task = 'regression',\n                             debug_log = 'automl_clvfuturespend_errors.log',\n                             primary_metric= 'normalized_root_mean_squared_error',\n                             iterations = 5,\n                             iteration_timeout_minutes = 5,\n                             X = X_train,\n                             y = y_train,\n                             n_cross_validations = 3,\n                             path=project_folder,\n                             verbosity = logging.INFO)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.2 Train your models on local compute\n\nWhen you call the submit method on the experiment object and pass the AutoMLConfig object, automated ML will general a number of machine learning models equal to **iterations** (5 in our case). Depending on the input data and the number of iterations the time required to complete the traning can range from a few minutes to hours (or even mode). Once execution starts, you will see status messages being print out to the console."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "local_run = experiment.submit(automl_config, show_output=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.3 Monitor training"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "When you want to start the run and continue to execute your code you need to specify ```show_output=False``` when calling experiment.submit. This will also enable you to use a widget to view the status of all iterations (see the cell below)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\nRunDetails(local_run).show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Section 4. Explore the results and test the best model"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 4.1 Retrieve all child runs\n\nEach individual model is trained in the context of a child run having **local_run** as its parent. You can get the list of all child runs and their logged metrics."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "children = list(local_run.get_children())\nmetricslist = {}\nfor run in children:\n    properties = run.get_properties()\n    metrics = {k: v for k, v in run.get_metrics().items() if isinstance(v, float)}    \n    metricslist[int(properties['iteration'])] = metrics\n\nrundata = pd.DataFrame(metricslist).sort_index(1)\nrundata",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 4.2 Retrieve the best fitted model\n\nThe **get_output** method enables you to retrieve the best child run and the associated fitted model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_run, fitted_model = local_run.get_output()\nfitted_model.steps",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 4.3 Test the best fitted model\n\nDescription of the process to be added."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Test model code to be added",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Section 5. Model Explainability: Which features matter for the predictions?\n\nDescription of the process to be added"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Model explainability code to be added",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}